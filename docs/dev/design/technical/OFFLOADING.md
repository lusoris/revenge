# Revenge - Advanced Offloading Architecture

<!-- SOURCES: dragonfly, fx, koanf, prometheus, prometheus-metrics, river, rueidis, rueidis-docs, typesense, typesense-go -->

<!-- DESIGN: technical, 01_ARCHITECTURE, 02_DESIGN_PRINCIPLES, 03_METADATA_SYSTEM -->


> Keep only essential services hot, offload everything else with fast spin-up.


<!-- TOC-START -->

## Table of Contents

- [Status](#status)
- [Philosophy](#philosophy)
- [Service Categories](#service-categories)
  - [ğŸ”´ Always Hot (Never Offload)](#always-hot-never-offload)
  - [ğŸŸ¡ Warm Standby (Connection Pooling)](#warm-standby-connection-pooling)
  - [ğŸŸ¢ Cold Start (Lazy Initialization)](#cold-start-lazy-initialization)
  - [âš« Background (Async Processing)](#background-async-processing)
- [Implementation Patterns](#implementation-patterns)
  - [1. Lazy Service Factory](#1-lazy-service-factory)
  - [2. Connection Pool with Auto-Scaling](#2-connection-pool-with-auto-scaling)
  - [3. Service Health & Readiness](#3-service-health-readiness)
  - [4. Graceful Degradation](#4-graceful-degradation)
- [Offloading by Module](#offloading-by-module)
  - [Playback Service](#playback-service)
  - [Content Modules (Movie, TV, etc.)](#content-modules-movie-tv-etc)
  - [Infrastructure](#infrastructure)
- [Memory Budget](#memory-budget)
- [Configuration](#configuration)
- [Monitoring](#monitoring)
  - [Metrics to Track](#metrics-to-track)
  - [Alerts](#alerts)
- [Sources & Cross-References](#sources-cross-references)
  - [Cross-Reference Indexes](#cross-reference-indexes)
  - [Referenced Sources](#referenced-sources)
- [Related Design Docs](#related-design-docs)
  - [In This Section](#in-this-section)
  - [Related Topics](#related-topics)
  - [Indexes](#indexes)
- [Best Practices](#best-practices)
  - [DO](#do)
  - [DON'T](#dont)

<!-- TOC-END -->

## Status

| Dimension | Status |
|-----------|--------|
| Design | ğŸ”´ |
| Sources | ğŸ”´ |
| Instructions | ğŸ”´ |
| Code | ğŸ”´ |
| Linting | ğŸ”´ |
| Unit Testing | ğŸ”´ |
| Integration Testing | ğŸ”´ |
---

## Philosophy

**Core Principle:** Minimize resource usage during idle periods while ensuring instant availability when needed.

| Category | Strategy | Target Latency |
|----------|----------|----------------|
| **Always Hot** | Never offload | 0ms |
| **Warm Standby** | Keep connections, pause processing | <10ms |
| **Cold Start** | Full offload, lazy initialization | <100ms |
| **Background** | Async, no latency requirements | N/A |

---

## Service Categories

### ğŸ”´ Always Hot (Never Offload)

These services must always be ready for instant response:

| Service | Reason |
|---------|--------|
| HTTP Server | User-facing API responses |
| Auth Middleware | Every request needs auth |
| Session Cache | Session validation on every request |
| Config | Needed by all services |

```go
// These are always initialized at startup
fx.Provide(
    NewHTTPServer,
    NewAuthMiddleware,
    NewSessionCache,
    config.Load,
)
```

### ğŸŸ¡ Warm Standby (Connection Pooling)

Keep connections alive, but minimize active processing:

| Service | Strategy |
|---------|----------|
| Database Pool | Maintain min connections, scale on demand |
| Redis/Dragonfly | Keep connection, lazy reconnect |
| Typesense | Connection pool with health checks |

```go
// Database with dynamic pool sizing
type DatabaseConfig struct {
    MinConns        int32         `koanf:"min_conns"`        // 2
    MaxConns        int32         `koanf:"max_conns"`        // 50
    MaxConnIdleTime time.Duration `koanf:"max_conn_idle"`    // 5m
    HealthCheckPeriod time.Duration `koanf:"health_check"`   // 30s
}
```

### ğŸŸ¢ Cold Start (Lazy Initialization)

Only initialize when first needed:

| Service | Trigger |
|---------|---------|
| Transcoder Client | First playback request |
| Metadata Providers | First library scan |
| Search Indexer | First index operation |
| Email Service | First notification |
| OIDC Providers | First SSO login |

```go
// Lazy initialization pattern
type LazyService[T any] struct {
    once     sync.Once
    instance T
    init     func() (T, error)
    err      error
}

func (l *LazyService[T]) Get() (T, error) {
    l.once.Do(func() {
        l.instance, l.err = l.init()
    })
    return l.instance, l.err
}
```

### âš« Background (Async Processing)

No latency requirements, can be fully offloaded:

| Service | Pattern |
|---------|---------|
| Library Scanner | River job queue |
| Metadata Fetcher | River job queue |
| Image Processor | River job queue |
| Search Reindexer | River job queue |
| Cleanup Tasks | Scheduled River jobs |
| Analytics | Batch processing |

---

## Implementation Patterns

### 1. Lazy Service Factory

```go
// pkg/lazy/lazy.go
package lazy

import (
    "sync"
    "sync/atomic"
)

// Service wraps a lazily-initialized service.
type Service[T any] struct {
    init     func() (T, error)
    instance atomic.Pointer[T]
    once     sync.Once
    err      error
}

// New creates a lazy service wrapper.
func New[T any](init func() (T, error)) *Service[T] {
    return &Service[T]{init: init}
}

// Get returns the service instance, initializing if needed.
func (s *Service[T]) Get() (T, error) {
    s.once.Do(func() {
        instance, err := s.init()
        if err != nil {
            s.err = err
            return
        }
        s.instance.Store(&instance)
    })

    if s.err != nil {
        var zero T
        return zero, s.err
    }

    return *s.instance.Load(), nil
}

// IsInitialized returns whether the service has been initialized.
func (s *Service[T]) IsInitialized() bool {
    return s.instance.Load() != nil
}
```

### 2. Connection Pool with Auto-Scaling

```go
// internal/infra/database/pool.go
type AdaptivePool struct {
    pool    *pgxpool.Pool
    config  AdaptivePoolConfig
    metrics PoolMetrics
    mu      sync.RWMutex
}

type AdaptivePoolConfig struct {
    MinConns           int32
    MaxConns           int32
    IdleTimeout        time.Duration
    ScaleUpThreshold   float64 // 0.8 = scale when 80% utilized
    ScaleDownThreshold float64 // 0.2 = scale down when 20% utilized
    ScaleCheckInterval time.Duration
}

func (p *AdaptivePool) adjustPoolSize() {
    stats := p.pool.Stat()
    utilization := float64(stats.AcquiredConns()) / float64(stats.MaxConns())

    if utilization > p.config.ScaleUpThreshold {
        // Increase pool size (handled by pgx automatically up to MaxConns)
        p.metrics.ScaleUpEvents++
    } else if utilization < p.config.ScaleDownThreshold {
        // Let idle connections timeout naturally
        p.metrics.ScaleDownEvents++
    }
}
```

### 3. Service Health & Readiness

```go
// internal/health/health.go
type HealthChecker struct {
    checks map[string]Check
    mu     sync.RWMutex
}

type Check struct {
    Name     string
    Category string // "critical", "warm", "cold"
    Check    func(ctx context.Context) error
    Timeout  time.Duration
}

type HealthStatus struct {
    Status   string            `json:"status"` // "healthy", "degraded", "unhealthy"
    Services map[string]Status `json:"services"`
}

type Status struct {
    Healthy     bool          `json:"healthy"`
    Initialized bool          `json:"initialized"`
    Latency     time.Duration `json:"latency_ms"`
    Error       string        `json:"error,omitempty"`
}
```

### 4. Graceful Degradation

```go
// When non-critical services fail, degrade gracefully
type DegradableService struct {
    primary   Service
    fallback  Service
    degraded  atomic.Bool
}

func (d *DegradableService) Execute(ctx context.Context, op Operation) (Result, error) {
    if d.degraded.Load() {
        return d.fallback.Execute(ctx, op)
    }

    result, err := d.primary.Execute(ctx, op)
    if err != nil && isCriticalError(err) {
        d.degraded.Store(true)
        go d.scheduleRecovery()
        return d.fallback.Execute(ctx, op)
    }

    return result, err
}
```

---

## Offloading by Module

### Playback Service

| Component | Strategy | Notes |
|-----------|----------|-------|
| Session Manager | Always Hot | Active sessions in memory |
| Transcode Cache | Warm | Memory-pressure aware eviction |
| Stream Buffer | Warm | Per-session, evict on idle |
| Transcoder Client | Cold | Init on first playback |
| Bandwidth Monitor | Warm | Only for active sessions |

### Content Modules (Movie, TV, etc.)

| Component | Strategy | Notes |
|-----------|----------|-------|
| Handler | Always Hot | HTTP handlers |
| Service | Warm | Business logic |
| Repository | Warm | Database queries |
| Scanner | Cold | Only during library scan |
| Provider | Cold | Only during metadata fetch |

### Infrastructure

| Component | Strategy | Notes |
|-----------|----------|-------|
| Database | Warm | Min 2 connections |
| Cache | Warm | Persistent connection |
| Search | Cold | Init on first search |
| Jobs (River) | Warm | Polling for jobs |

---

## Memory Budget

Target memory allocation by category:

| Category | Budget | Notes |
|----------|--------|-------|
| HTTP/API | 50MB | Request handling |
| Session Cache | 100MB | Active sessions |
| Transcode Cache | 25% RAM | Configurable |
| Database Pool | 50MB | Connection overhead |
| River Workers | 100MB | Job processing |
| Misc | 200MB | Buffers, etc. |

**Total Baseline:** ~500MB + 25% RAM for transcode cache

---

## Configuration

```yaml
# configs/offloading.yaml
offloading:
  # Connection pools
  database:
    min_conns: 2
    max_conns: 50
    idle_timeout: 5m

  cache:
    pool_size: 10
    min_idle: 2

  # Lazy services
  lazy_init:
    transcoder: true
    metadata_providers: true
    email: true
    oidc: true

  # Memory limits
  memory:
    transcode_cache_percent: 25
    session_cache_max_mb: 100

  # Health checks
  health:
    interval: 30s
    timeout: 5s
```

---

## Monitoring

### Metrics to Track

```go
type OffloadingMetrics struct {
    // Service initialization
    LazyInitCount     prometheus.Counter
    LazyInitLatency   prometheus.Histogram

    // Connection pools
    PoolActiveConns   prometheus.Gauge
    PoolIdleConns     prometheus.Gauge
    PoolWaitCount     prometheus.Counter

    // Memory
    HeapInUse         prometheus.Gauge
    TranscodeCacheSize prometheus.Gauge

    // Degradation
    DegradedServices  prometheus.Gauge
    FallbackCount     prometheus.Counter
}
```

### Alerts

| Metric | Threshold | Action |
|--------|-----------|--------|
| Pool utilization | >90% | Scale up |
| Lazy init latency | >500ms | Investigate |
| Degraded services | >0 | Alert on-call |
| Memory pressure | >80% | Evict caches |


---

## Best Practices

### DO

- âœ… Use lazy initialization for non-critical services
- âœ… Implement graceful degradation
- âœ… Monitor initialization latency
- âœ… Set appropriate connection pool limits
- âœ… Use memory-pressure-aware caching
- âœ… Background process heavy operations
- âœ… Health check all dependencies

### DON'T

- âŒ Eagerly initialize everything at startup
- âŒ Keep unused connections open indefinitely
- âŒ Fail hard when optional services are down
- âŒ Ignore memory pressure signals
- âŒ Block request handling for background work
- âŒ Over-allocate connection pools
