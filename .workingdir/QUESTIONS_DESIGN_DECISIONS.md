# Design-Entscheidungen & Offene Fragen

**Erstellt**: 2026-02-05
**Kontext**: Fokus auf Clusterf√§higkeit, Security-Fixes, und Deduplication ohne Monolith

---

## 1. Clusterf√§higkeit

### 1.1 Media-File-Storage

**Problem**: Media-Dateien sind aktuell auf lokalem Filesystem, nicht cluster-tauglich.

**Optionen**:
- **Option A: NFS/CephFS/SMB** (ReadOnlyMany Kubernetes Volume)
  - ‚úÖ Native Filesystem-Performance
  - ‚úÖ Keine Code-√Ñnderungen n√∂tig
  - ‚ùå Externe NFS-Server-Infrastruktur n√∂tig

- **Option B: Object Storage** (S3/MinIO)
  - ‚úÖ Cloud-native, hochverf√ºgbar
  - ‚úÖ Keine NFS-Infrastruktur
  - ‚ùå Gro√üer Refactoring-Aufwand (Scanner umschreiben)
  - ‚ùå Schlechtere Performance f√ºr Streaming

- **Option C: Hybrid** (NFS f√ºr Media, S3 f√ºr Avatars)
  - ‚úÖ Beste Performance wo n√∂tig
  - ‚ùå Zwei Storage-Systeme zu verwalten

**Frage**: Welche Storage-L√∂sung bevorzugst du f√ºr Media-Dateien?

---

### 1.2 Avatar/User-Content-Storage

**Problem**: Avatar-Uploads aktuell auf lokalem Filesystem.

**Optionen**:
- **Option A: S3/MinIO** (Empfohlen)
  - ‚úÖ Cloud-native, einfach skalierbar
  - ‚úÖ Interface bereits vorbereitet
  - ‚úÖ 8-16h Implementierungsaufwand

- **Option B: Shared Volume** (NFS mit ReadWriteMany)
  - ‚úÖ Einfacher wenn NFS schon da ist
  - ‚ùå RWX-Volumes komplexer als RO

**Frage**: S3/MinIO f√ºr Avatars implementieren?

---

### 1.3 Leader Election f√ºr Periodic Jobs

**Problem**: Cleanup-Jobs w√ºrden auf jedem Pod laufen (Mehrfachausf√ºhrung).

**Optionen**:
- **Option A: River Periodic Jobs** (Empfohlen)
  - ‚úÖ Native River-Feature
  - ‚úÖ Einfache Integration
  - ‚úÖ PostgreSQL-basiert (kein extra Service)
  - ‚úÖ 4-8h Aufwand

- **Option B: HashiCorp Raft**
  - ‚úÖ Bereits im go.mod
  - ‚ùå Komplexer Setup
  - ‚ùå Overkill f√ºr unseren Use-Case

- **Option C: Kubernetes Lease API**
  - ‚úÖ Native Kubernetes-Feature
  - ‚ùå Nur f√ºr K8s, nicht f√ºr Docker Swarm

- **Option D: PostgreSQL Advisory Locks**
  - ‚úÖ Einfach, kein extra Service
  - ‚ùå Manuell implementieren

**Frage**: River Periodic Jobs f√ºr Leader Election verwenden?

---

## 2. TV Shows Implementation

### 2.1 Metadata Provider

**Kontext**: TMDb unterst√ºtzt sowohl Filme als auch TV-Serien!

**Optionen**:
- **Option A: TMDb f√ºr TV Shows nutzen** (Empfohlen)
  - ‚úÖ TMDb-Client bereits vorhanden
  - ‚úÖ Einheitliche API
  - ‚úÖ Weniger Dependencies
  - ‚úÖ Schnellere Implementierung
  - ‚ùå M√∂glicherweise weniger TV-spezifische Features als TheTVDB

- **Option B: TheTVDB zus√§tzlich**
  - ‚úÖ Spezialisiert auf TV-Daten
  - ‚úÖ Community-Daten
  - ‚ùå Neuer Client n√∂tig
  - ‚ùå Zwei Systeme zu pflegen

- **Option C: Beide (TMDb prim√§r, TheTVDB fallback)**
  - ‚úÖ Beste Datenqualit√§t
  - ‚ùå Komplexere Logik

**Frage**: Nur TMDb f√ºr TV Shows verwenden oder zus√§tzlich TheTVDB?

---

### 2.2 TV Shows Timing

**Frage**: Wann soll TV-Implementierung starten?
- Direkt nach Cluster + Security-Fixes?
- Oder erst nach vollst√§ndiger Deduplication/Refactoring?

---

## 3. Shared Abstractions & Deduplication

### 3.1 Tabellen-Architektur

**Prinzip**: Module eigenst√§ndig, Tabellen NICHT monolithisch!

**M√∂gliche Ans√§tze**:

**Option A: Separate Schemas pro Content-Typ** (Empfohlen)
```sql
-- Movies
public.movies
public.movie_files
public.movie_genres
public.movie_credits

-- TV Shows
tvshow.series
tvshow.seasons
tvshow.episodes
tvshow.episode_files

-- Music
music.artists
music.albums
music.tracks
music.track_files
```
‚úÖ Klare Trennung
‚úÖ Unabh√§ngige Migrationen
‚úÖ Keine Kollisionen

**Option B: Prefix-basiert in public Schema**
```sql
public.movies
public.movie_files
public.tvshows
public.tvshow_episodes
public.music_albums
public.music_tracks
```
‚úÖ Einfacher
‚ùå Weniger Isolation

**Option C: Separate Datenbanken**
```
revenge_movies
revenge_tvshows
revenge_music
```
‚ùå Overkill
‚ùå Cross-Module-Queries unm√∂glich

**Frage**: Separate Schemas (Option A) oder Prefix-basiert (Option B)?

---

### 3.2 Shared Code vs. Eigenst√§ndigkeit

**Zu teilen** (OK, kein Monolith):
- ‚úÖ Algorithmen (Levenshtein, Fuzzy Matching)
- ‚úÖ File-Scanner-Framework (mit Adaptern)
- ‚úÖ HTTP-Provider-Framework (mit Adaptern)
- ‚úÖ Background-Job-Boilerplate

**NICHT zu teilen** (w√ºrde Monolith erzeugen):
- ‚ùå Domain-Models (Movie, TVShow, Album bleiben getrennt)
- ‚ùå Repositories (jedes Modul hat eigenes)
- ‚ùå Database-Tabellen
- ‚ùå API-Endpoints

**Frage**: Ist diese Trennung OK f√ºr dich?

---

### 3.3 Refactoring-Timing

**Option A: Jetzt sofort refactoren**
- Shared Abstractions extrahieren
- Movie-Modul umbauen
- Dann TV implementieren
- ‚è±Ô∏è 4-6 Wochen vor TV-Start

**Option B: Schrittweise**
- TV mit aktuellem Pattern implementieren (wie Movie)
- Danach beide refactoren
- ‚è±Ô∏è TV schneller, aber mehr Duplikation

**Option C: Minimal jetzt, aggressiv sp√§ter**
- Nur kritische Utils extrahieren (Levenshtein, Patterns)
- TV implementieren
- Gro√ües Refactoring wenn 3+ Module da sind

**Frage**: Wann und wie aggressiv sollen wir refactoren?

---

## 4. Security-Fixes

### 4.1 Transaktionen

**Betroffene Stellen**:
1. User-Registrierung (`CreateUser` + `CreateEmailVerificationToken`)
2. Avatar-Upload (mehrere DB-Operationen)
3. Session-Refresh (`CreateSession` + `RevokeSession`)

**Frage**: Alle auf einmal fixen oder iterativ?

---

### 4.2 Timing-Attack im Login

**Fix**: Dummy-Hash-Vergleich auch wenn User nicht existiert

**Frage**:
- Argon2id-Parameter beibehalten? (memory=64MB, time=3, parallelism=2)
- Oder anpassen f√ºr bessere Performance/Security-Balance?

---

### 4.3 Account-Lockout

**Optionen**:
- **Option A: Service-Layer Lockout**
  - DB-Tabelle f√ºr Failed-Attempts
  - Lockout nach 5 Versuchen
  - 15-30 Minuten Sperre

- **Option B: Redis-basiert**
  - Schneller
  - Automatisches Expiry
  - Aber: State in Redis

**Frage**: Service-Layer mit DB oder Redis-basiert?

---

## 5. Deployment & Infrastructure

### 5.1 Deployment-Target

**Frage**: Welche Plattformen sind prim√§r?
- [ ] Kubernetes (self-hosted)
- [ ] k3s (Lightweight K8s)
- [ ] Docker Swarm
- [ ] Docker Compose (Development)
- [ ] Bare Metal (direkt auf Server)

---

### 5.2 Monitoring & Observability

**Aktuell**:
- ‚úÖ Prometheus-Metriken
- ‚úÖ Strukturiertes Logging
- üü° OpenTelemetry importiert, aber nicht instrumentiert

**Frage**:
- Distributed Tracing jetzt implementieren oder sp√§ter?
- Welche Monitoring-Stack bevorzugt? (Prometheus/Grafana, ELK, andere?)

---

## 6. Testing

### 6.1 Test-Coverage-Ziel

**Aktuell**: 41% (Movie), aber Ziel ist 80%

**Frage**:
- Erst 80% erreichen, dann neue Features?
- Oder parallel: neue Features mit 80% Coverage schreiben?

---

### 6.2 Integration-Test-Strategy

**Frage**:
- Testcontainers f√ºr alle Services? (PostgreSQL, Dragonfly, Typesense)
- Oder Mocks f√ºr schnellere Tests?

---

## 7. Migration-Path

### Priorit√§ten-Reihenfolge

**Vorschlag**:
1. **Woche 1-2**: Security-Fixes (Kritisch)
   - Transaktionen
   - Timing-Attacks
   - Goroutine-Leaks

2. **Woche 3-4**: Cluster-Readiness (Blocker f√ºr Production)
   - Media-Storage (NFS)
   - Avatar-Storage (S3)
   - Leader-Election (River)

3. **Woche 5-6**: TODOs in existierenden Modulen
   - IP/User-Agent Extraktion
   - SendGrid API
   - Async Reindex

4. **Woche 7-8**: Shared Abstractions
   - Scanner-Framework
   - Matcher-Framework

5. **Woche 9+**: TV Shows Implementation

**Frage**: Ist diese Reihenfolge OK oder andere Priorit√§ten?

---

## 8. TV Shows Spezifisch

### 8.1 TMDb TV API Struktur

**TMDb TV Endpoints**:
- `/tv/{id}` - Series Details
- `/tv/{id}/season/{season_number}` - Season Details
- `/tv/{id}/season/{season_number}/episode/{episode_number}` - Episode

**Frage**: Separate Tabellen f√ºr Series/Seasons/Episodes oder flachere Struktur?

```sql
-- Option A: Hierarchisch (empfohlen)
tvshow.series (id, tmdb_id, title, ...)
tvshow.seasons (id, series_id, season_number, ...)
tvshow.episodes (id, season_id, episode_number, title, ...)
tvshow.episode_files (id, episode_id, path, ...)

-- Option B: Flacher
tvshow.episodes (id, series_id, season_number, episode_number, ...)
tvshow.episode_files (id, episode_id, ...)
```

---

### 8.2 Filename-Parsing f√ºr TV

**Patterns**:
- `Series.Name.S01E05.Episode.Title.1080p.mkv`
- `Series Name - 1x05 - Episode Title.mkv`
- `Series.Name.105.Episode.Title.mkv` (season 1, episode 5)

**Frage**: Welche Naming-Conventions sind wichtig f√ºr dich?

---

## 9. Sonarr Integration

**Frage**:
- Sonarr-Integration gleichzeitig mit TV-Shows?
- Oder erst TV-Modul, dann sp√§ter Sonarr?

---

## Zusammenfassung: Wichtigste Entscheidungen

**Bitte priorisieren (1 = h√∂chste Priorit√§t)**:

1. [ ] **Storage-L√∂sung** (NFS vs S3 vs Hybrid)
2. [ ] **Leader Election** (River Periodic vs andere)
3. [ ] **TMDb f√ºr TV** (ja/nein/zus√§tzlich TheTVDB)
4. [ ] **Tabellen-Architektur** (Separate Schemas vs Prefix)
5. [ ] **Refactoring-Timing** (jetzt vs schrittweise vs sp√§ter)
6. [ ] **Security-Fix-Scope** (alle sofort vs iterativ)
7. [ ] **Deployment-Priorit√§t** (K8s vs k3s vs Swarm)
8. [ ] **Test-Coverage** (erst 80% dann Features vs parallel)
9. [ ] **TV-Timing** (nach Cluster-Fixes vs nach Refactoring)
10. [ ] **Sonarr-Timing** (mit TV vs sp√§ter)
