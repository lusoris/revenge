# A8.1.1 Implementation: NFS Volume Support for Media

**Status**: ✅ COMPLETE
**Date**: 2026-02-05
**Priority**: P0 (Critical - Cluster Readiness)

## Overview

Implemented shared media storage support for cluster deployments using NFS volumes. Supports three configuration modes: direct NFS mount, existing PVC, or automatic PVC creation.

## Changes Made

### 1. Helm Chart Configuration

#### values.yaml
Added media persistence configuration:

```yaml
media:
  persistence:
    enabled: false
    storageClass: nfs-client
    accessMode: ReadOnlyMany
    size: 1Ti
    existingClaim: ''
    nfs:
      server: ''
      path: ''
      readOnly: true
  moviePaths:
  - /media/movies
  tvPaths:
  - /media/tv
  musicPaths:
  - /media/music
```

**Configuration Options**:
- `enabled`: Toggle media volume mounting
- `storageClass`: K8s StorageClass for automatic PVC
- `accessMode`: ReadOnlyMany for multiple pods
- `size`: Volume size (default 1Ti)
- `existingClaim`: Use pre-created PVC
- `nfs.server`: Direct NFS server address
- `nfs.path`: NFS export path
- `nfs.readOnly`: Mount as read-only (recommended)

### 2. Deployment Template

#### templates/deployment.yaml
Added volume configuration with three modes:

**Volume Definition** (spec.template.spec.volumes):
1. **Existing PVC Mode**: Uses pre-created PVC
2. **Direct NFS Mode**: Mounts NFS directly
3. **Auto PVC Mode**: Creates PVC from template

**Volume Mount** (containers[0].volumeMounts):
- Mounts `/media` as read-only
- Only added when `media.persistence.enabled=true`

### 3. PersistentVolumeClaim Template

#### templates/media-pvc.yaml (NEW)
Automatically creates PVC when:
- `media.persistence.enabled=true`
- No `existingClaim` specified
- No direct NFS mount configured

Features:
- Uses configured StorageClass
- ReadOnlyMany access mode
- Configurable size
- Labeled for Helm management

### 4. Example PersistentVolume

#### examples/media-pv-nfs.yaml (NEW)
Reference PV configuration showing:
- NFS server/path configuration
- ReadOnlyMany access mode
- Retain reclaim policy
- NFS mount options (nfsvers=4.1, hard, retrans)

### 5. Docker Compose NFS Override

#### docker-compose.nfs.yml (NEW)
Override file for NFS in Docker deployments:
- Replaces local `./media` mount with NFS volume
- Uses Docker volume driver with NFS backend
- Environment variables for NFS_SERVER and NFS_PATH
- Includes recommended NFS mount options

Usage:
```bash
NFS_SERVER=nas.example.com NFS_PATH=/volume1/media \
  docker-compose -f docker-compose.yml -f docker-compose.nfs.yml up
```

### 6. Documentation

#### charts/revenge/README.md (NEW)
Comprehensive documentation covering:
- Three configuration modes with pros/cons
- Installation examples for each mode
- Media path configuration
- StorageClass setup (nfs-subdir-external-provisioner)
- Troubleshooting guide
- Configuration reference table

## Architecture

### Storage Modes Comparison

| Mode | Use Case | Complexity | GitOps-Friendly |
|------|----------|------------|-----------------|
| Direct NFS | Quick setup, single cluster | Low | No (credentials in values) |
| Existing PVC | Pre-provisioned storage | Medium | Yes (separate storage mgmt) |
| Auto PVC | Dynamic provisioning | Medium-High | Yes (fully declarative) |

### Media Mount Flow

```
Kubernetes Volume
       ↓
NFS Server (nas.example.com:/volume1/media)
       ↓
Pod: /media (read-only)
       ↓
Application reads:
  - /media/movies → configured in media.moviePaths
  - /media/tv → configured in media.tvPaths
  - /media/music → configured in media.musicPaths
```

### High Availability

**ReadOnlyMany Access**:
- Multiple pods can read simultaneously
- No locking issues
- Perfect for horizontal scaling

**Media Consistency**:
- All pods see same files (NFS shared)
- No need for file replication
- Single source of truth

## Testing

### Manual Validation Checklist

- [ ] Helm chart syntax valid (run `helm lint`)
- [ ] Direct NFS mount works in test cluster
- [ ] Existing PVC mode works
- [ ] Auto PVC creation works
- [ ] Multiple replicas can access media
- [ ] Read-only mount enforced
- [ ] Docker Compose NFS override works
- [ ] Documentation accurate

### Test Scenarios

**Scenario 1: Direct NFS Mount**
```bash
helm install revenge ./charts/revenge \
  --set media.persistence.enabled=true \
  --set media.persistence.nfs.server=192.168.1.100 \
  --set media.persistence.nfs.path=/exports/media

kubectl exec -it deployment/revenge -- ls /media/movies
# Should list movies from NFS server
```

**Scenario 2: Multiple Replicas**
```bash
kubectl scale deployment/revenge --replicas=3
# All 3 pods should mount /media successfully
```

**Scenario 3: Docker Compose NFS**
```bash
NFS_SERVER=192.168.1.100 NFS_PATH=/exports/media \
  docker-compose -f docker-compose.yml -f docker-compose.nfs.yml up
```

## Benefits

### Cluster-Ready
- **Multiple Replicas**: All pods access same media
- **Horizontal Scaling**: Add/remove pods without storage issues
- **No Data Replication**: Single NFS source, no sync needed

### Deployment Flexibility
- **Three Configuration Modes**: Choose based on environment
- **Docker Compose Support**: Works outside Kubernetes too
- **Storage Agnostic**: NFS, CephFS, any ReadOnlyMany storage

### Operational Excellence
- **Read-Only Mount**: Prevents accidental modifications
- **Documented**: Clear examples and troubleshooting
- **GitOps-Friendly**: Declarative configuration (modes 2 & 3)

## Configuration Examples

### Production with Auto-Scaling

```yaml
replicaCount: 3

media:
  persistence:
    enabled: true
    nfs:
      server: nas.prod.example.com
      path: /volume1/media
      readOnly: true

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70
```

### Development with Local Storage

```yaml
replicaCount: 1

media:
  persistence:
    enabled: false  # Use local ./media mount from docker-compose
```

### Multi-Cluster with Pre-Provisioned PVC

```yaml
media:
  persistence:
    enabled: true
    existingClaim: media-shared-nfs-pvc
    # PVC created by infrastructure team
```

## Migration Path

### From Single-Node to Cluster

1. **Current State**: Single pod, local media mount
2. **Setup NFS**: Export media directory via NFS
3. **Update Values**: Enable media.persistence with NFS config
4. **Scale Up**: Increase replicaCount
5. **Verify**: All pods access same media

### From Local Docker Compose to Kubernetes

1. **Export Media**: Share directory via NFS
2. **Test NFS**: Verify Docker NFS override works
3. **Deploy Helm**: Use same NFS server/path in values
4. **Migrate Traffic**: Switch ingress/load balancer
5. **Decommission**: Remove old docker-compose stack

## Future Enhancements

### A8.1.2: S3/MinIO for Avatars
- Move user-generated content (avatars) to S3
- Keep media (movies/TV) on NFS (better performance)
- Hybrid storage model

### Potential Improvements
- Support for multiple NFS servers (movies, TV on different mounts)
- ReadWriteMany support for media management tools
- Integration with storage provisioners (Rook-Ceph, Longhorn)
- Metrics on media access patterns

## References

- Kubernetes NFS Volumes: https://kubernetes.io/docs/concepts/storage/volumes/#nfs
- PersistentVolumes: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
- NFS Provisioner: https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner
- Docker NFS Volumes: https://docs.docker.com/storage/volumes/#use-a-volume-driver

---

**Phase A8.1.1 Complete**: NFS media storage support implemented for cluster deployments.
