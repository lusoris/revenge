doc_title: Babepedia Integration
doc_category: integration
created_date: '2026-01-31'
overall_status: âœ… Complete
status_design: âœ…
status_design_notes: '-'
status_sources: âœ…
status_sources_notes: '-'
status_instructions: âœ…
status_instructions_notes: '-'
status_code: ðŸ”´
status_code_notes: '-'
status_linting: ðŸ”´
status_linting_notes: '-'
status_unit_testing: ðŸ”´
status_unit_testing_notes: '-'
status_integration_testing: ðŸ”´
status_integration_testing_notes: '-'
technical_summary: '> Adult performer wiki with biographies and filmographies'
wiki_tagline: '> Performer biographies from Babepedia'
wiki_overview: Babepedia provides biographical information for adult performers. Physical attributes, career history, and
  social media links. Supplements StashDB performer data. Used within the QAR adult content system. Links to performer profiles
  from content detail pages.
sources:
- name: Dragonfly Documentation
  url: https://www.dragonflydb.io/docs
  note: Auto-resolved from dragonfly
- name: Go io
  url: https://pkg.go.dev/io
  note: Auto-resolved from go-io
- name: River Job Queue
  url: https://pkg.go.dev/github.com/riverqueue/river
  note: Auto-resolved from river
design_refs:
- title: 01_ARCHITECTURE
  path: ../../../architecture/01_ARCHITECTURE.md
- title: 02_DESIGN_PRINCIPLES
  path: ../../../architecture/02_DESIGN_PRINCIPLES.md
- title: 03_METADATA_SYSTEM
  path: ../../../architecture/03_METADATA_SYSTEM.md
integration_name: Babepedia
integration_id: babepedia
external_service: Babepedia
api_base_url: https://www.babepedia.com
auth_method: none
architecture_diagram: |-
  ```mermaid
  flowchart TD
      node1["Revenge<br/>QAR Module<br/>(Performer)"]
      node2["Babepedia<br/>Scraper"]
      node3["Performer Data<br/>- Biography"]
      node4["Rate Limiter"]
      node2 --> node3
      node1 --> node2
      node3 --> node4
  ```
api_details: |
  **Method**: Web scraping (no official API)
  **Base URL**: `https://www.babepedia.com`
  **Authentication**: None required
  **Rate Limit**: Self-imposed (1 req/2sec to be polite)

  **URL Patterns**:
  - Performer profile: `/babe/{performer_name}`
  - Search: `/search.php?q={query}`
  - Letter index: `/babes/letter/{letter}`

  **Data Available**:
  - Full name and aliases
  - Date of birth / Age
  - Birthplace / Nationality
  - Physical measurements (height, weight, measurements)
  - Career start/end years
  - Social media links
  - Profile photo

  **Scraping Notes**:
  - HTML structure is consistent
  - Profile info in structured `<div>` elements
  - Handle URL encoding for names with spaces/special chars
database_schema: |
  ```sql
  -- Babepedia performer cache (QAR schema)
  CREATE TABLE qar.babepedia_performers (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    babepedia_url TEXT UNIQUE NOT NULL,
    performer_name VARCHAR(255) NOT NULL,
    aliases TEXT[],
    birth_date DATE,
    birthplace VARCHAR(255),
    nationality VARCHAR(100),
    height_cm INT,
    measurements VARCHAR(50),
    hair_color VARCHAR(50),
    eye_color VARCHAR(50),
    career_start INT,
    career_end INT,
    social_links JSONB,
    photo_url TEXT,
    fetched_at TIMESTAMPTZ DEFAULT now(),
    expires_at TIMESTAMPTZ NOT NULL
  );
  CREATE INDEX idx_babepedia_name ON qar.babepedia_performers(performer_name);
  CREATE INDEX idx_babepedia_expires ON qar.babepedia_performers(expires_at);

  -- Performer to Babepedia mapping
  CREATE TABLE qar.performer_babepedia_links (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    performer_id UUID REFERENCES qar.performers(id) ON DELETE CASCADE,
    babepedia_url TEXT NOT NULL,
    match_confidence FLOAT,
    created_at TIMESTAMPTZ DEFAULT now(),
    UNIQUE(performer_id)
  );
  ```
module_structure: |
  ```
  internal/metadata/providers/babepedia/
  â”œâ”€â”€ module.go                    # fx module
  â”œâ”€â”€ client.go                    # HTTP client with proxy support
  â”œâ”€â”€ scraper.go                   # HTML scraping logic
  â”œâ”€â”€ parser.go                    # Profile data extraction
  â”œâ”€â”€ provider.go                  # Enrichment provider
  â”œâ”€â”€ search.go                    # Name search functionality
  â”œâ”€â”€ matching.go                  # Name fuzzy matching
  â””â”€â”€ babepedia_test.go
  ```
key_interfaces: |
  ```go
  // Babepedia enrichment provider
  type BabepediaProvider struct {
    httpClient  *httpclient.Client  // From HTTP_CLIENT service
    rateLimiter *rate.Limiter
    cache       Cache
    parser      *goquery.Document
  }

  // Performer enrichment provider interface
  type PerformerEnrichmentProvider interface {
    SearchPerformer(ctx context.Context, name string) ([]PerformerResult, error)
    GetPerformer(ctx context.Context, url string) (*PerformerProfile, error)
    MatchPerformer(ctx context.Context, name string, aliases []string) (*PerformerProfile, error)
    Priority() int  // Returns 30 (after StashDB)
  }

  // Babepedia performer profile
  type PerformerProfile struct {
    URL          string    `json:"url"`
    Name         string    `json:"name"`
    Aliases      []string  `json:"aliases"`
    BirthDate    *time.Time `json:"birth_date,omitempty"`
    Birthplace   string    `json:"birthplace"`
    Nationality  string    `json:"nationality"`
    HeightCm     int       `json:"height_cm"`
    Measurements string    `json:"measurements"`
    HairColor    string    `json:"hair_color"`
    EyeColor     string    `json:"eye_color"`
    CareerStart  int       `json:"career_start"`
    CareerEnd    int       `json:"career_end,omitempty"`
    SocialLinks  map[string]string `json:"social_links"`
    PhotoURL     string    `json:"photo_url"`
  }

  // HTML parsing with goquery
  func (p *BabepediaProvider) parseProfile(doc *goquery.Document) (*PerformerProfile, error) {
    profile := &PerformerProfile{}

    // Extract structured data from profile page
    doc.Find(".profbox li").Each(func(i int, s *goquery.Selection) {
      label := s.Find(".profhead").Text()
      value := s.Find(".profdata").Text()

      switch strings.TrimSuffix(label, ":") {
      case "Born":
        profile.BirthDate = parseDate(value)
      case "Birthplace":
        profile.Birthplace = value
      case "Height":
        profile.HeightCm = parseHeight(value)
      // ... etc
      }
    })

    return profile, nil
  }
  ```
dependencies: |
  **Go Packages**:
  - `github.com/PuerkitoBio/goquery` - HTML parsing
  - `golang.org/x/time/rate` - Rate limiting
  - `github.com/jackc/pgx/v5` - PostgreSQL
  - `github.com/riverqueue/river` - Background jobs
  - `go.uber.org/fx` - DI

  **Internal**:
  - `internal/service/httpclient` - Proxy-enabled HTTP client

  **External**:
  - Babepedia website (no official API)
env_vars: |
  ```bash
  BABEPEDIA_ENABLED=true
  BABEPEDIA_RATE_LIMIT=0.5          # req/sec (1 every 2 seconds)
  BABEPEDIA_CACHE_TTL=720h          # 30 days
  BABEPEDIA_USE_PROXY=true          # Use proxy pool from HTTP_CLIENT
  ```
config_keys: |
  ```yaml
  qar:
    metadata:
      providers:
        babepedia:
          enabled: true
          rate_limit: 0.5
          rate_window: 1s
          cache_ttl: 720h
          role: enrichment
          priority: 30             # After StashDB (10), Boobpedia (20)
          use_proxy: true
          retry_on_block: true
  ```
component_interaction: |
  **Performer Enrichment Flow**:
  1. Performer identified via Whisparr/StashDB
  2. Background job checks for additional data
  3. Search Babepedia by performer name
  4. Parse search results for best match
  5. Scrape full profile page
  6. Extract measurements, bio, social links
  7. Store in cache with 30-day TTL
  8. Link performer record to Babepedia data

  **Data Merge Strategy**:
  - StashDB is PRIMARY for performer identity
  - Babepedia SUPPLEMENTS with:
    - Detailed measurements
    - Career timeline
    - Social media links
    - Better photos in some cases
  - Merge non-conflicting data only
enrichment_role: |
  **ENRICHMENT-Only Provider**

  Babepedia supplements primary performer data:
  - Physical measurements and statistics
  - Career start/end dates
  - Social media profile links
  - Alternative name spellings

  **Not used for primary identification**:
  - StashDB for performer identity/fingerprinting
  - IAFD for filmography/scene credits

  **When to Query**:
  - After performer identified by StashDB
  - When user views performer detail page
  - Background enrichment for library
scraping_implementation: |
  **Robust Scraping Pattern**:
  ```go
  func (p *BabepediaProvider) scrapeProfile(ctx context.Context, url string) (*PerformerProfile, error) {
    // Rate limit
    if err := p.rateLimiter.Wait(ctx); err != nil {
      return nil, err
    }

    // Use proxy-enabled client
    resp, err := p.httpClient.Get(ctx, url)
    if err != nil {
      return nil, fmt.Errorf("fetch failed: %w", err)
    }
    defer resp.Body.Close()

    // Check for blocks/captchas
    if resp.StatusCode == 403 || resp.StatusCode == 429 {
      return nil, ErrRateLimited
    }

    // Parse HTML
    doc, err := goquery.NewDocumentFromReader(resp.Body)
    if err != nil {
      return nil, fmt.Errorf("parse failed: %w", err)
    }

    return p.parseProfile(doc)
  }
  ```
error_handling: |
  **HTTP Errors**:
  - 403 Forbidden â†’ IP blocked, rotate proxy
  - 404 Not Found â†’ Performer not on Babepedia
  - 429 Too Many Requests â†’ Rate limited, back off
  - 5xx â†’ Service error, retry with backoff

  **Scraping Errors**:
  - Structure changed â†’ Log and skip field
  - Missing data â†’ Return partial result
  - Encoding issues â†’ Handle UTF-8 properly
unit_tests: |
  ```go
  func TestBabepedia_ParseProfile(t *testing.T) {
    html := `<div class="profbox">
      <li><span class="profhead">Born:</span><span class="profdata">January 1, 1990</span></li>
      <li><span class="profhead">Height:</span><span class="profdata">5'6" (168 cm)</span></li>
    </div>`
    doc, _ := goquery.NewDocumentFromReader(strings.NewReader(html))

    provider := NewBabepediaProvider(nil)
    profile, err := provider.parseProfile(doc)

    require.NoError(t, err)
    assert.Equal(t, 168, profile.HeightCm)
  }

  func TestBabepedia_NameMatching(t *testing.T) {
    provider := NewBabepediaProvider(nil)

    // Test with aliases
    match := provider.matchName("Jane Doe", []string{"J. Doe", "Jane D."})
    assert.True(t, match)
  }
  ```
best_practices: |
  **Scraping Etiquette**:
  - Respect rate limits (1 req/2sec)
  - Use proxy rotation to avoid blocks
  - Cache aggressively (30 days)
  - Handle graceful degradation

  **Data Quality**:
  - Validate parsed data before storing
  - Cross-reference with other sources
  - Flag uncertain matches for review

  **Privacy**:
  - All data flows through QAR access controls
  - Requires `legacy:read` scope
  - Data isolated in qar.* schema
