doc_title: River Integration
doc_category: integration
created_date: '2026-01-31'
overall_status: âœ… Complete
status_design: âœ…
status_design_notes: '-'
status_sources: âœ…
status_sources_notes: '-'
status_instructions: âœ…
status_instructions_notes: '-'
status_code: ðŸ”´
status_code_notes: '-'
status_linting: ðŸ”´
status_linting_notes: '-'
status_unit_testing: ðŸ”´
status_unit_testing_notes: '-'
status_integration_testing: ðŸ”´
status_integration_testing_notes: '-'
technical_summary: '> PostgreSQL-native job queue'
wiki_tagline: '> Background job processing with River'
wiki_overview: River handles background tasks like library scanning, metadata fetching, and transcoding. Jobs are stored in
  PostgreSQL for reliability - nothing is lost if the server restarts. View job status in the admin dashboard. Configure worker
  concurrency based on your server capacity.
sources:
- name: Dragonfly Documentation
  url: https://www.dragonflydb.io/docs
  note: Auto-resolved from dragonfly
- name: Uber fx
  url: https://pkg.go.dev/go.uber.org/fx
  note: Auto-resolved from fx
- name: google/uuid
  url: https://pkg.go.dev/github.com/google/uuid
  note: Auto-resolved from google-uuid
- name: pgx PostgreSQL Driver
  url: https://pkg.go.dev/github.com/jackc/pgx/v5
  note: Auto-resolved from pgx
- name: pgxpool Connection Pool
  url: https://pkg.go.dev/github.com/jackc/pgx/v5/pgxpool
  note: Auto-resolved from pgxpool
- name: PostgreSQL Arrays
  url: https://www.postgresql.org/docs/current/arrays.html
  note: Auto-resolved from postgresql-arrays
- name: PostgreSQL JSON Functions
  url: https://www.postgresql.org/docs/current/functions-json.html
  note: Auto-resolved from postgresql-json
- name: Prometheus Go Client
  url: https://pkg.go.dev/github.com/prometheus/client_golang/prometheus
  note: Auto-resolved from prometheus
- name: Prometheus Metric Types
  url: https://prometheus.io/docs/concepts/metric_types/
  note: Auto-resolved from prometheus-metrics
- name: River Job Queue
  url: https://pkg.go.dev/github.com/riverqueue/river
  note: Auto-resolved from river
- name: River Documentation
  url: https://riverqueue.com/docs
  note: Auto-resolved from river-docs
- name: rueidis
  url: https://pkg.go.dev/github.com/redis/rueidis
  note: Auto-resolved from rueidis
- name: rueidis GitHub README
  url: https://github.com/redis/rueidis
  note: Auto-resolved from rueidis-docs
- name: Typesense API
  url: https://typesense.org/docs/latest/api/
  note: Auto-resolved from typesense
- name: Typesense Go Client
  url: https://github.com/typesense/typesense-go
  note: Auto-resolved from typesense-go
design_refs:
- title: 01_ARCHITECTURE
  path: ../../architecture/01_ARCHITECTURE.md
- title: 02_DESIGN_PRINCIPLES
  path: ../../architecture/02_DESIGN_PRINCIPLES.md
- title: 03_METADATA_SYSTEM
  path: ../../architecture/03_METADATA_SYSTEM.md
integration_name: River
integration_id: river
external_service: River
architecture_diagram: |-
  ```mermaid
  flowchart TD
      subgraph row1[ ]
          direction LR
          node1[["Services<br/>(Enqueue)"]]
          node2(["River<br/>Client"])
          node3[("PostgreSQL<br/>(Job Store)")]
      end
      node4["Workers<br/>(Background)"]
      node1 --> node2
      node2 --> node3
      node3 --> node4

      %% Hide row subgraph borders
      style row1 fill:transparent,stroke:transparent
  ```
connection_details: |
  **Job Queue**: River (PostgreSQL-native background job processor)
  **Storage**: Jobs stored in PostgreSQL `river_job` schema
  **Client**: riverqueue/river Go package
  **Worker Pool**: Configurable concurrency per job type

  **Key Features**:
  - PostgreSQL-native (no Redis/RabbitMQ needed)
  - Job persistence (survives server restarts)
  - Priority queues
  - Scheduled/delayed jobs
  - Retry with exponential backoff
  - Job cancellation
  - Leadership election for distributed workers
job_architecture: |
  **Job Lifecycle**:
  1. **Enqueue**: Service creates job, inserted into `river_job` table
  2. **Available**: Job becomes available at scheduled time
  3. **Running**: Worker picks up job and executes
  4. **Completed**: Job finishes successfully
  5. **Failed**: Job fails, retried with backoff (max 25 attempts)
  6. **Discarded**: Job exceeds retry limit

  **Job States**:
  - `available` - Ready to run
  - `running` - Currently executing
  - `completed` - Successfully finished
  - `retryable` - Failed, will retry
  - `scheduled` - Waiting for scheduled time
  - `cancelled` - Manually cancelled
  - `discarded` - Permanently failed
database_schema: |
  **Schema**: `river_job` (managed by River)

  ```sql
  -- Job queue table (managed by River)
  CREATE TABLE river_job (
    id BIGSERIAL PRIMARY KEY,
    state VARCHAR(50) NOT NULL,
    queue VARCHAR(100) NOT NULL,
    kind VARCHAR(100) NOT NULL,
    args JSONB NOT NULL,
    priority INTEGER DEFAULT 1,
    scheduled_at TIMESTAMPTZ NOT NULL,
    attempted_at TIMESTAMPTZ,
    attempt INTEGER DEFAULT 0,
    max_attempts INTEGER DEFAULT 25,
    errors TEXT[],
    created_at TIMESTAMPTZ DEFAULT now(),
    finalized_at TIMESTAMPTZ
  );
  CREATE INDEX idx_river_job_state_queue ON river_job(state, queue);
  CREATE INDEX idx_river_job_scheduled ON river_job(scheduled_at);

  -- Leadership election (managed by River)
  CREATE TABLE river_leader (
    name VARCHAR(100) PRIMARY KEY,
    leader_id VARCHAR(100) NOT NULL,
    elected_at TIMESTAMPTZ NOT NULL,
    expires_at TIMESTAMPTZ NOT NULL
  );
  ```
module_structure: |
  ```
  internal/jobs/
  â”œâ”€â”€ module.go                    # fx module
  â”œâ”€â”€ client.go                    # River client
  â”œâ”€â”€ workers/                     # Job workers
  â”‚   â”œâ”€â”€ library_scan.go
  â”‚   â”œâ”€â”€ metadata_fetch.go
  â”‚   â”œâ”€â”€ transcode.go
  â”‚   â”œâ”€â”€ analytics_aggregate.go
  â”‚   â”œâ”€â”€ activity_cleanup.go
  â”‚   â””â”€â”€ ...
  â””â”€â”€ jobs_test.go
  ```
key_interfaces: |
  ```go
  // River client wrapper
  type JobQueue interface {
    Enqueue(ctx context.Context, job river.JobArgs) (*river.JobInsertResult, error)
    EnqueueWithPriority(ctx context.Context, job river.JobArgs, priority int) (*river.JobInsertResult, error)
    EnqueueScheduled(ctx context.Context, job river.JobArgs, scheduledAt time.Time) (*river.JobInsertResult, error)
    Cancel(ctx context.Context, jobID int64) error
    Start(ctx context.Context) error
    Stop(ctx context.Context) error
  }

  // Example worker
  type LibraryScanWorker struct {
    river.WorkerDefaults[LibraryScanArgs]
    libraryService LibraryService
  }

  type LibraryScanArgs struct {
    LibraryID uuid.UUID `json:"library_id"`
    FullScan  bool      `json:"full_scan"`
  }

  func (w *LibraryScanWorker) Work(ctx context.Context, job *river.Job[LibraryScanArgs]) error {
    return w.libraryService.ScanLibrary(ctx, job.Args.LibraryID, job.Args.FullScan)
  }
  ```
dependencies: |
  **Go Packages**:
  - `github.com/riverqueue/river` - Job queue
  - `github.com/riverqueue/river/riverdriver/riverpgxv5` - PostgreSQL driver
  - `github.com/jackc/pgx/v5/pgxpool` - Connection pool
  - `github.com/google/uuid` - UUID support
  - `go.uber.org/fx` - Dependency injection
env_vars: |
  ```bash
  # River configuration
  RIVER_WORKERS=10
  RIVER_MAX_ATTEMPTS=25
  RIVER_POLL_INTERVAL=1s
  RIVER_SHUTDOWN_TIMEOUT=30s

  # Queue priorities
  RIVER_QUEUE_DEFAULT_PRIORITY=1
  RIVER_QUEUE_HIGH_PRIORITY=10
  ```
config_keys: |
  ```yaml
  jobs:
    river:
      workers: 10
      max_attempts: 25
      poll_interval: 1s
      shutdown_timeout: 30s

      queues:
        default:
          max_workers: 10
        high_priority:
          max_workers: 20
        low_priority:
          max_workers: 5
  ```
component_interaction: |
  **Enqueue Job**:
  1. Service needs to scan library
  2. Create LibraryScanArgs{LibraryID: uuid}
  3. Call jobQueue.Enqueue(ctx, args)
  4. River inserts job into river_job table
  5. Job becomes available immediately

  **Execute Job** (background):
  1. River worker polls for available jobs
  2. Worker picks up job based on priority and queue
  3. Worker executes LibraryScanWorker.Work()
  4. If successful, mark job as completed
  5. If failed, increment attempt and retry with backoff

  **Scheduled Job** (delayed execution):
  1. Enqueue job with future scheduled_at timestamp
  2. Job stays in `scheduled` state until time reached
  3. Job becomes `available` at scheduled time
  4. Worker picks up and executes normally
job_examples: |
  **Library Scan Job**:
  ```go
  type LibraryScanArgs struct {
    LibraryID uuid.UUID `json:"library_id"`
    FullScan  bool      `json:"full_scan"`
  }

  // Enqueue
  jobQueue.Enqueue(ctx, LibraryScanArgs{
    LibraryID: libraryID,
    FullScan:  false,
  })
  ```

  **Metadata Fetch Job** (with retry):
  ```go
  type MetadataFetchArgs struct {
    ContentID   uuid.UUID `json:"content_id"`
    ProviderID  string    `json:"provider_id"`
  }

  // Worker with custom retry logic
  func (w *MetadataFetchWorker) Work(ctx context.Context, job *river.Job[MetadataFetchArgs]) error {
    err := w.metadataService.Fetch(ctx, job.Args.ContentID, job.Args.ProviderID)
    if err != nil {
      // Retry on transient errors
      if isTransient(err) {
        return err
      }
      // Don't retry on permanent errors (e.g., 404)
      return river.JobCancel(err)
    }
    return nil
  }
  ```

  **Scheduled Analytics Job** (daily aggregation):
  ```go
  // Schedule for next midnight
  nextMidnight := time.Now().Add(24 * time.Hour).Truncate(24 * time.Hour)

  jobQueue.EnqueueScheduled(ctx, AnalyticsAggregateArgs{
    Date: time.Now(),
  }, nextMidnight)
  ```
api_endpoints: |
  **List Jobs**:
  ```
  GET /api/v1/admin/jobs?state=running&limit=50
  ```

  **Response**:
  ```json
  {
    "jobs": [
      {
        "id": 12345,
        "state": "running",
        "queue": "default",
        "kind": "library_scan",
        "args": {
          "library_id": "uuid-123",
          "full_scan": false
        },
        "attempt": 1,
        "max_attempts": 25,
        "created_at": "2026-02-01T10:00:00Z",
        "scheduled_at": "2026-02-01T10:00:00Z"
      }
    ],
    "total": 1
  }
  ```

  **Cancel Job**:
  ```
  DELETE /api/v1/admin/jobs/:id
  ```
common_jobs: |
  **Background Jobs in Revenge**:
  - `library_scan` - Scan library for new/changed files
  - `metadata_fetch` - Fetch metadata from external providers
  - `transcode` - Video transcoding for streaming
  - `trickplay_generate` - Generate timeline thumbnails
  - `intro_detect` - Detect intro/credits segments
  - `analytics_aggregate` - Daily analytics aggregation
  - `activity_cleanup` - Clean up old activity logs
  - `session_cleanup` - Expire old sessions
  - `notification_send` - Send email/push notifications
  - `data_export` - GDPR data export
  - `user_deletion` - GDPR user deletion
retry_strategy: |
  **Retry Configuration**:
  - Default max attempts: 25
  - Backoff: Exponential with jitter
  - Initial delay: 30 seconds
  - Max delay: 24 hours

  **Backoff Formula**:
  ```
  delay = min(30s * 2^attempt + jitter, 24h)
  ```

  **Example Retry Schedule**:
  - Attempt 1: 30s
  - Attempt 2: 1m
  - Attempt 3: 2m
  - Attempt 4: 4m
  - Attempt 5: 8m
  - ...
  - Attempt 25: 24h (max)
unit_tests: |
  ```go
  func TestRiverClient_Enqueue(t *testing.T) {
    // Test job enqueueing
  }

  func TestWorker_LibraryScan(t *testing.T) {
    // Test worker execution
  }
  ```
integration_tests: |
  ```go
  func TestRiver_FullWorkflow(t *testing.T) {
    // Use testcontainers for PostgreSQL
    // Enqueue job, verify execution, check completion
  }
  ```
monitoring: |
  **Metrics Exposed** (via Prometheus):
  - `river_jobs_enqueued_total` - Total jobs enqueued
  - `river_jobs_completed_total` - Total jobs completed
  - `river_jobs_failed_total` - Total jobs failed
  - `river_jobs_running` - Currently running jobs
  - `river_job_duration_seconds` - Job execution time histogram
  - `river_worker_utilization` - Worker pool utilization

  **Dashboard Queries**:
  - Job success rate: `rate(river_jobs_completed_total[5m]) / rate(river_jobs_enqueued_total[5m])`
  - Average job duration: `rate(river_job_duration_seconds_sum[5m]) / rate(river_job_duration_seconds_count[5m])`
performance_tuning: |
  **Worker Sizing**:
  - Set `workers` to ~2x CPU cores for I/O-bound jobs
  - For CPU-bound jobs (transcoding), set to CPU cores
  - Monitor worker utilization and adjust

  **Queue Prioritization**:
  - Use high priority queue for user-facing jobs (metadata fetch)
  - Use low priority queue for background maintenance (analytics)

  **Job Batching**:
  - For bulk operations, enqueue jobs in batches with `InsertMany`
  - Reduces transaction overhead
